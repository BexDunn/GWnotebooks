{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TCI code implementing dask for maximum spatial extent and tracking memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#TCI_shapefile1.py\n",
    "\n",
    "''' \n",
    "This code loads in surface reflectance data from the data cube, calculates \n",
    "tasselled cap indices, and outputs a netcdf file.\n",
    "Created by Bex Dunn 08/05/2017\n",
    "'''\n",
    "#get some libraries\n",
    "import datacube\n",
    "import xarray as xr\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "import json\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import numpy as np #need this for pq fuser\n",
    "\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "from datacube.storage.masking import mask_valid_data as mask_invalid_data\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "#dealing with system commands\n",
    "import sys\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file 'mptest.txt'. \n"
     ]
    }
   ],
   "source": [
    "#%load_ext memory_profiler\n",
    "%mprun -T 'mptest.txt' -f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is :0\n"
     ]
    }
   ],
   "source": [
    "#code to work with a polygon input rather than a lat/long box\n",
    "# #pick a shape file\n",
    "shape_file = ('/g/data/r78/rjd547/groundwater_activities/Burdekin_shapefiles/burd_dam/burd_dam_noZ.shp')\n",
    "# open all the shapes within the shape file\n",
    "shapes = fiona.open(shape_file)\n",
    "\n",
    "i=0\n",
    "print('i is :'+str(i))\n",
    "if i > len(shapes):\n",
    "    print('index not in the range for the shapefile: '+str(i)+' not in '+str(len(shapes)))\n",
    "    sys.exit(0)\n",
    "\n",
    "#code to take in the system argument i.e. the number of the polygon to use.\n",
    "#copy attributes from shapefile and define shape_name\n",
    "geom_crs = geometry.CRS(shapes.crs_wkt)\n",
    "geo = shapes[i]['geometry']\n",
    "geom = geometry.Geometry(geo, crs=geom_crs)\n",
    "geom_bs = shapely.geometry.shape(shapes[i]['geometry'])\n",
    "shape_name = shape_file.split('/')[-1].split('.')[0]+'_'+str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(left=402430.4822999984, bottom=-2240082.9014999997, right=435008.5001000017, top=-2201792.9518999998)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geom.boundingbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crs': 'PROJCS[\"GABWRA_Albers_Equal_Area_Conic\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"WGS_1984\",SPHEROID[\"WGS_84\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"False_Easting\",0.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"longitude_of_center\",143.0],PARAMETER[\"Standard_Parallel_1\",-21.0],PARAMETER[\"Standard_Parallel_2\",-29.0],PARAMETER[\"latitude_of_center\",0.0],UNIT[\"Meter\",1.0]]',\n",
       " 'x': (402430.4822999984, 435008.5001000017),\n",
       " 'y': (-2201792.9518999998, -2240082.9014999997)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_q = {\n",
    "    'x': (geom.boundingbox.left, geom.boundingbox.right), \n",
    "    'y': (geom.boundingbox.top, geom.boundingbox.bottom),\n",
    "    'crs': geom.crs.wkt,\n",
    "    }\n",
    "spatial_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.context.set_options at 0x7f5f5b70d8d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "dask.set_options(get=dask.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tell the datacube which app to use\n",
    "dc = datacube.Datacube(app='dc-nbar')\n",
    "\n",
    "#### DEFINE SPATIOTEMPORAL RANGE AND BANDS OF INTEREST\n",
    "#Use this to manually define an upper left/lower right coords\n",
    "#Either as polygon or as lat/lon range\n",
    "\n",
    "\n",
    "#Define temporal range\n",
    "start_of_epoch = '1987-01-01'\n",
    "#need a variable here that defines a rolling 'latest observation'\n",
    "end_of_epoch =  '2016-12-31'\n",
    "\n",
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = ['blue',\n",
    "                     'green',\n",
    "                     'red', \n",
    "                     'nir',\n",
    "                     'swir1', \n",
    "                     'swir2'\n",
    "                     ]\n",
    "\n",
    "#Define sensors of interest\n",
    "sensor1 = 'ls5'\n",
    "sensor2 = 'ls7'\n",
    "sensor3 = 'ls8'\n",
    "\n",
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch), # 'geopolygon': geom\n",
    "    'dask_chunks': {'time': 5},\n",
    "}\n",
    "query.update(spatial_q)\n",
    "\n",
    "#Group PQ by solar day to avoid idiosyncracies of N/S overlap differences in PQ algorithm performance\n",
    "pq_albers_product = dc.index.products.get_by_name(sensor1+'_pq_albers')\n",
    "valid_bit = pq_albers_product.measurements['pixelquality']['flags_definition']['contiguous']['bits']\n",
    "\n",
    "def pq_fuser(dest, src):\n",
    "    valid_val = (1 << valid_bit)\n",
    "\n",
    "    no_data_dest_mask = ~(dest & valid_val).astype(bool)\n",
    "    np.copyto(dest, src, where=no_data_dest_mask)\n",
    "\n",
    "    both_data_mask = (valid_val & dest & src).astype(bool)\n",
    "    np.copyto(dest, src & dest, where=both_data_mask)\n",
    "\n",
    "wetness_coeff = {}\n",
    "wetness_coeff['ls5'] = (0.151, 0.179, 0.330, 0.341, -0.711, -0.457)\n",
    "wetness_coeff['ls7'] = (0.151, 0.179, 0.330, 0.341, -0.711, -0.457)\n",
    "#wetness_coeff['ls7'] = (0.2626, 0.2141, 0.0926, 0.0656, -0.7629, -0.5388)\n",
    "wetness_coeff['ls8'] = (0.1511, 0.1973, 0.3283, 0.3407, -0.7117, -0.4559)\n",
    "\n",
    "\n",
    "## PQ and Index preparation\n",
    "\n",
    "\n",
    "# retrieve the NBAR and PQ for the spatiotemporal range of interest\n",
    "\n",
    "\n",
    "#Retrieve the NBAR and PQ data for sensor n\n",
    "sensor1_nbar = dc.load(product= sensor1+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor1_pq = dc.load(product= sensor1+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)\n",
    "           \n",
    "crs = sensor1_nbar.crs\n",
    "crswkt = sensor1_nbar.crs.wkt\n",
    "affine = sensor1_nbar.affine\n",
    "\n",
    "#Generate PQ masks and apply those masks to remove cloud, cloud shadow, saturated observations\n",
    "s1_cloud_free = masking.make_mask(sensor1_pq, \n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s1_good_data = s1_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor1_nbar = sensor1_nbar.where(s1_good_data)\n",
    "sensor1_nbar.attrs['crs'] = crs\n",
    "sensor1_nbar.attrs['affine'] = affine\n",
    "\n",
    "sensor2_nbar = dc.load(product= sensor2+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor2_pq = dc.load(product= sensor2+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)                  \n",
    "\n",
    "s2_cloud_free = masking.make_mask(sensor2_pq, \n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s2_good_data = s2_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor2_nbar = sensor2_nbar.where(s2_good_data)\n",
    "sensor2_nbar.attrs['crs'] = crs\n",
    "sensor2_nbar.attrs['affine'] = affine\n",
    "\n",
    "sensor3_nbar = dc.load(product= sensor3+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor3_pq = dc.load(product= sensor3+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)                  \n",
    "\n",
    "s3_cloud_free = masking.make_mask(sensor3_pq, \n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s3_good_data = s3_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor3_nbar = sensor3_nbar.where(s3_good_data)\n",
    "sensor3_nbar.attrs['crs'] = crs\n",
    "sensor3_nbar.attrs['affine'] = affine\n",
    "\n",
    "nbar_clean = xr.concat([sensor1_nbar, sensor2_nbar, sensor3_nbar], dim='time')\n",
    "time_sorted = nbar_clean.time.argsort()\n",
    "nbar_clean = nbar_clean.isel(time=time_sorted)\n",
    "\n",
    "#Calculate Taselled Cap Wetness\n",
    "wetness_sensor1_nbar = ((sensor1_nbar.blue*wetness_coeff[sensor1][0])+(sensor1_nbar.green*wetness_coeff[sensor1][1])+\n",
    "                          (sensor1_nbar.red*wetness_coeff[sensor1][2])+(sensor1_nbar.nir*wetness_coeff[sensor1][3])+\n",
    "                          (sensor1_nbar.swir1*wetness_coeff[sensor1][4])+(sensor1_nbar.swir2*wetness_coeff[sensor1][5]))\n",
    "wetness_sensor2_nbar = ((sensor2_nbar.blue*wetness_coeff[sensor2][0])+(sensor2_nbar.green*wetness_coeff[sensor2][1])+\n",
    "                          (sensor2_nbar.red*wetness_coeff[sensor2][2])+(sensor2_nbar.nir*wetness_coeff[sensor2][3])+\n",
    "                          (sensor2_nbar.swir1*wetness_coeff[sensor2][4])+(sensor2_nbar.swir2*wetness_coeff[sensor2][5]))\n",
    "wetness_sensor3_nbar = ((sensor3_nbar.blue*wetness_coeff[sensor3][0])+(sensor3_nbar.green*wetness_coeff[sensor3][1])+\n",
    "                          (sensor3_nbar.red*wetness_coeff[sensor3][2])+(sensor3_nbar.nir*wetness_coeff[sensor3][3])+\n",
    "                          (sensor3_nbar.swir1*wetness_coeff[sensor3][4])+(sensor3_nbar.swir2*wetness_coeff[sensor3][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wet_thresh = -400 #may need to adapt this based on aridity/location\n",
    "\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_1 = wetness_sensor1_nbar.count(dim = 'time')\n",
    "\n",
    "#set wetness data threshold. catch warning about using numpy inequalities.. RuntimeWarning: invalid value encountered in greaterif not reflexive\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    wat_plus_wetv_1=wetness_sensor1_nbar.where(wetness_sensor1_nbar>wet_thresh)\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold for each pixel\n",
    "threshold_count_1=wat_plus_wetv_1.count(dim='time')\n",
    "\n",
    "#divide the number of times wetness is seen by the number of wetness scenes to get a proportion of time that the \n",
    "#pixel is wet or wet veg'd:\n",
    "new_wet_count_1= threshold_count_1/wet_count_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wet_count_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Do it all again, because concatenating it makes it hard to do the mask filter 'where'\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_2 = wetness_sensor2_nbar.count(dim = 'time')\n",
    "\n",
    "#set wetness data threshold. catch warning about using numpy inequalities.. RuntimeWarning: invalid value encountered in greaterif not reflexive\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    wat_plus_wetv_2=wetness_sensor2_nbar.where(wetness_sensor2_nbar>wet_thresh)\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold for each pixel\n",
    "threshold_count_2=wat_plus_wetv_2.count(dim='time')\n",
    "\n",
    "#divide the number of times wetness is seen by the number of wetness scenes to get a proportion of time that the \n",
    "#pixel is wet or wet veg'd:\n",
    "new_wet_count_2= threshold_count_2/wet_count_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Do it all again, because concatenating it makes it hard to do the mask filter 'where'\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_3 = wetness_sensor3_nbar.count(dim = 'time')\n",
    "\n",
    "#set wetness data threshold. catch warning about using numpy inequalities.. RuntimeWarning: invalid value encountered in greaterif not reflexive\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    wat_plus_wetv_3=wetness_sensor3_nbar.where(wetness_sensor3_nbar>wet_thresh)\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold for each pixel\n",
    "threshold_count_3=wat_plus_wetv_3.count(dim='time')\n",
    "\n",
    "#divide the number of times wetness is seen by the number of wetness scenes to get a proportion of time that the \n",
    "#pixel is wet or wet veg'd:\n",
    "new_wet_count_3= threshold_count_3/wet_count_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_wet_count = dask.array.stack([new_wet_count_1,new_wet_count_2,new_wet_count_3])\n",
    "new_wet_count = dask.array.sum(new_wet_count, axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load new_wet_count into memory so we can plot it and write it to netcd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(new_wet_count, cmap = 'gist_earth_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Output wet_count to netCDF\n",
    "\n",
    "# #get the original dataset attributes (crs)\n",
    "# #set up variable attributes to hold the attributes from sensor1_nbar\n",
    "# attrs = sensor1_nbar\n",
    "# #dump the extra data to just get the attributes\n",
    "# nbar_data = attrs.data_vars.keys()\n",
    "# for j in nbar_data:\n",
    "#     #drop band data, retaining just the attributes\n",
    "#     attrs =attrs.drop(j)\n",
    "# #set up new variable called wet_vars, and assign attributes to it in a dictionary\n",
    "# wet_vars = {'wet_count':''}\n",
    "# wet_count_data = attrs.assign(**wet_vars)\n",
    "# wet_count_data['wet_count'] = wet_count\n",
    "\n",
    "# ncpath = '/g/data/r78/rjd547/groundwater_activities/GalileeBasin/Gal_AOI_5k_Raijin/'\n",
    "\n",
    "# try:\n",
    "#     write_dataset_to_netcdf(wet_count_data,variable_params={'wet_count': {'zlib':True}}, filename=ncpath+shape_name+'_run01.nc')\n",
    "# except RuntimeError as err:\n",
    "#     print(\"RuntimeError: {0}\".format(err))\n",
    "    \n",
    "    \n",
    "# print('successfully ran TCI for '+shape_name+' polygon number '+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file <ipython-input-42-f6c03c205705>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n",
      "ERROR: Could not find file <ipython-input-42-f6c03c205705>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@profile\n",
    "def test1():\n",
    "    n = 10000\n",
    "    a = [1] * n\n",
    "    time.sleep(1)\n",
    "    return a\n",
    "\n",
    "@profile\n",
    "def test2():\n",
    "    n = 100000\n",
    "    b = [1] * n\n",
    "    time.sleep(1)\n",
    "    return b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test1()\n",
    "    test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
