{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reincorporates debugging workflows to add into raijin script and tests that they work.\n",
    "\n",
    "This notebooks runs the whole of northern australia script that produced the pizza slice anomalies on a small shapefile that intersects the boundary of an anomaly. \n",
    "Then it plots an image of the result.\n",
    "\n",
    "This is Dask_Wetness_nbart.py as run across the whole of northern australia, but run only on a single small shapefile created to highlight an anomalous region.\n",
    "Bex 31/07/2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20170427/envs/agdc/lib/python3.6/site-packages/dask/array/core.py:2544: RuntimeWarning: invalid value encountered in greater\n",
      "  return function(*args2, **kwargs)\n",
      "/g/data/v10/public/modules/agdc-py3-env/20170427/envs/agdc/lib/python3.6/site-packages/dask/array/core.py:2544: RuntimeWarning: invalid value encountered in greater\n",
      "  return function(*args2, **kwargs)\n",
      "/g/data/v10/public/modules/agdc-py3-env/20170427/envs/agdc/lib/python3.6/site-packages/dask/array/core.py:2544: RuntimeWarning: invalid value encountered in greater\n",
      "  return function(*args2, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Storage Unit already exists: /g/data/r78/rjd547/groundwater_activities/Analysis/slice_0.nc\n",
      "successfully ran TCI for slice_0 polygon number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "successfully ran TCI for slice_0 polygon number 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully wrote overthresh netCDF for slice_0 polygon number 0\n",
      "successfully wrote clearobs netCDF for slice_0 polygon number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "successfully wrote overthresh netCDFfor slice_0 polygon number 0\n",
      "successfully wrote clearobs netCDFfor slice_0 polygon number 0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#Dask_Wetness_nbarT.py\n",
    "\n",
    "'''\n",
    "Dask_Wetness_nbarT.py loads surface reflectance data from the data cube, calculates\n",
    "tasselled cap indices, and outputs a netcdf file. It uses dask to keep memory use low.\n",
    "--It requires a PBS submission script to provide i, the polygon number to ingest.\n",
    "Created by Bex Dunn 08/05/2017\n",
    "Altered to increase the size of the query, to test dask run times, 16/06/17\n",
    "Altered to use the correct terrain masking nan values 30/06/17\n",
    "Altered to just use the polygon size again 30/06/17 #FIXME will need to remove the spatial query code (commented out)\n",
    "Altered to import new ga_pq_fuser function\n",
    "And to import mask_valid_data as mask_invalid_data as of 010812017\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#for writing to error files:\n",
    "from __future__ import print_function\n",
    "#get some libraries\n",
    "import datacube\n",
    "import xarray as xr\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "import json\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import numpy as np #need this for pq fuser\n",
    "\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "#from datacube.storage.masking import mask_invalid_data\n",
    "from datacube.helpers import ga_pq_fuser as pq_fuser\n",
    "from datacube.storage.masking import mask_valid_data as mask_invalid_data\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "#dealing with system commands\n",
    "import sys\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "#save netcdf outputs to this folder:\n",
    "netcdf_output_loc ='/g/data/r78/rjd547/groundwater_activities/Analysis/'\n",
    "\n",
    "#code to work with a polygon input rather than a lat/long box\n",
    "# #pick a shape file\n",
    "shape_file = ('/g/data/r78/rjd547/groundwater_activities/Analysis/slice.shp')\n",
    "# open all the shapes within the shape file\n",
    "shapes = fiona.open(shape_file)\n",
    "\n",
    "#i is the number of the polygon within the shapefile that the script will run for.\n",
    "#the next line takes i  as a system argument input eg. $Dask_Wetness_nbarT.py $1\n",
    "#We have to minus one here because python counts from 0 and bash counts from one, therefore\n",
    "#node 1 will be polygon 0.\n",
    "i=0\n",
    "#i=int(sys.argv[1])-1\n",
    "#i=5373\n",
    "print('i is :'+str(i))\n",
    "#if we have requested an i greater than the amount of polygons in the file, just print an error message\n",
    "#exit with success condition (0) not failure condition (anything else)\n",
    "if i > len(shapes):\n",
    "    print('index not in the range for the shapefile: '+str(i)+' not in '+str(len(shapes)))\n",
    "    sys.exit(0)\n",
    "\n",
    "#copy attributes from shapefile and define shape_name\n",
    "geom_crs = geometry.CRS(shapes.crs_wkt)\n",
    "geo = shapes[i]['geometry']\n",
    "geom = geometry.Geometry(geo, crs=geom_crs)\n",
    "geom_bs = shapely.geometry.shape(shapes[i]['geometry'])\n",
    "shape_name = shape_file.split('/')[-1].split('.')[0]+'_'+str(i)\n",
    "\n",
    "#bring in dask and start it running\n",
    "import dask\n",
    "dask.set_options(get=dask.get)\n",
    "\n",
    "#tell the datacube which app to use\n",
    "dc = datacube.Datacube(app='dc-nbar')\n",
    "\n",
    "#### DEFINE SPATIOTEMPORAL RANGE AND BANDS OF INTEREST\n",
    "#Define temporal range\n",
    "start_of_epoch = '1987-01-01'\n",
    "#need a variable here that defines a rolling 'latest observation'\n",
    "end_of_epoch =  '2016-12-31'\n",
    "\n",
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = ['blue',\n",
    "                     'green',\n",
    "                     'red',\n",
    "                     'nir',\n",
    "                     'swir1',\n",
    "                     'swir2'\n",
    "                     ]\n",
    "\n",
    "#Define sensors of interest\n",
    "sensor1 = 'ls5'\n",
    "sensor2 = 'ls7'\n",
    "sensor3 = 'ls8'\n",
    "\n",
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch), 'geopolygon': geom,\n",
    "    'dask_chunks': {'time': 5},\n",
    "}\n",
    "\n",
    "wetness_coeff = {}\n",
    "wetness_coeff['ls5'] = (0.151, 0.179, 0.330, 0.341, -0.711, -0.457)\n",
    "wetness_coeff['ls7'] = (0.151, 0.179, 0.330, 0.341, -0.711, -0.457)\n",
    "#wetness_coeff['ls7'] = (0.2626, 0.2141, 0.0926, 0.0656, -0.7629, -0.5388)\n",
    "wetness_coeff['ls8'] = (0.1511, 0.1973, 0.3283, 0.3407, -0.7117, -0.4559)\n",
    "\n",
    "#Retrieve the NBAR and PQ data for sensor n\n",
    "sensor1_nbar = dc.load(product= sensor1+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor1_pq = dc.load(product= sensor1+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)\n",
    "\n",
    "crs = sensor1_nbar.crs\n",
    "crswkt = sensor1_nbar.crs.wkt\n",
    "affine = sensor1_nbar.affine\n",
    "\n",
    "#Generate PQ masks and apply those masks to remove cloud, cloud shadow, saturated observations\n",
    "s1_cloud_free = masking.make_mask(sensor1_pq,\n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s1_good_data = s1_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor1_nbar = sensor1_nbar.where(s1_good_data)\n",
    "sensor1_nbar.attrs['crs'] = crs\n",
    "sensor1_nbar.attrs['affine'] = affine\n",
    "\n",
    "sensor2_nbar = dc.load(product= sensor2+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor2_pq = dc.load(product= sensor2+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)\n",
    "\n",
    "s2_cloud_free = masking.make_mask(sensor2_pq,\n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s2_good_data = s2_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor2_nbar = sensor2_nbar.where(s2_good_data)\n",
    "sensor2_nbar.attrs['crs'] = crs\n",
    "sensor2_nbar.attrs['affine'] = affine\n",
    "\n",
    "sensor3_nbar = dc.load(product= sensor3+'_nbart_albers', group_by='solar_day', measurements = bands_of_interest,  **query)\n",
    "sensor3_pq = dc.load(product= sensor3+'_pq_albers', group_by='solar_day', fuse_func=pq_fuser, **query)\n",
    "\n",
    "s3_cloud_free = masking.make_mask(sensor3_pq,\n",
    "                              cloud_acca='no_cloud',\n",
    "                              cloud_shadow_acca = 'no_cloud_shadow',\n",
    "                              cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                              cloud_fmask='no_cloud',\n",
    "                              blue_saturated = False,\n",
    "                              green_saturated = False,\n",
    "                              red_saturated = False,\n",
    "                              nir_saturated = False,\n",
    "                              swir1_saturated = False,\n",
    "                              swir2_saturated = False,\n",
    "                              contiguous=True)\n",
    "s3_good_data = s3_cloud_free.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "sensor3_nbar = sensor3_nbar.where(s3_good_data)\n",
    "sensor3_nbar.attrs['crs'] = crs\n",
    "sensor3_nbar.attrs['affine'] = affine\n",
    "\n",
    "#__________This section included so nbarT is correctly used to correct terrain by removing -999.0 values and replacing with nans________________________\n",
    "sensor1_nbar = sensor1_nbar.where(sensor1_nbar!=-999.0)\n",
    "sensor2_nbar = sensor2_nbar.where(sensor2_nbar!=-999.0)\n",
    "sensor3_nbar = sensor3_nbar.where(sensor3_nbar!=-999.0)\n",
    "\n",
    "#Calculate Taselled Cap Wetness\n",
    "wetness_sensor1_nbar = ((sensor1_nbar.blue*wetness_coeff[sensor1][0])+\n",
    "                        (sensor1_nbar.green*wetness_coeff[sensor1][1])+\n",
    "                        (sensor1_nbar.red*wetness_coeff[sensor1][2])+\n",
    "                        (sensor1_nbar.nir*wetness_coeff[sensor1][3])+\n",
    "                        (sensor1_nbar.swir1*wetness_coeff[sensor1][4])+\n",
    "                        (sensor1_nbar.swir2*wetness_coeff[sensor1][5])\n",
    "                       )\n",
    "wetness_sensor2_nbar = ((sensor2_nbar.blue*wetness_coeff[sensor2][0])+(sensor2_nbar.green*wetness_coeff[sensor2][1])+\n",
    "                          (sensor2_nbar.red*wetness_coeff[sensor2][2])+(sensor2_nbar.nir*wetness_coeff[sensor2][3])+\n",
    "                          (sensor2_nbar.swir1*wetness_coeff[sensor2][4])+(sensor2_nbar.swir2*wetness_coeff[sensor2][5]))\n",
    "wetness_sensor3_nbar = ((sensor3_nbar.blue*wetness_coeff[sensor3][0])+(sensor3_nbar.green*wetness_coeff[sensor3][1])+\n",
    "                          (sensor3_nbar.red*wetness_coeff[sensor3][2])+(sensor3_nbar.nir*wetness_coeff[sensor3][3])+\n",
    "                          (sensor3_nbar.swir1*wetness_coeff[sensor3][4])+(sensor3_nbar.swir2*wetness_coeff[sensor3][5]))\n",
    "\n",
    "threshold = -400\n",
    "\n",
    "#for wetness_sensor1_nbar, filter, count, and load.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #water_plus_wetveg is wetness values where wetness>threshold\n",
    "    water_plus_wetveg_1 = wetness_sensor1_nbar.where(wetness_sensor1_nbar>threshold)\n",
    "\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_1 = wetness_sensor1_nbar.count(dim='time')\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold\n",
    "threshold_count_1= water_plus_wetveg_1.count(dim='time')\n",
    "\n",
    "#bring both counts into memory\n",
    "wet_count_1.load()\n",
    "threshold_count_1.load()\n",
    "\n",
    "#for wetness_sensor2_nbar, filter, count, and load.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    water_plus_wetveg_2 = wetness_sensor2_nbar.where(wetness_sensor2_nbar>threshold)\n",
    "\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_2 = wetness_sensor2_nbar.count(dim='time')\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold\n",
    "threshold_count_2= water_plus_wetveg_2.count(dim='time')\n",
    "\n",
    "#bring both counts into memory\n",
    "wet_count_2.load()\n",
    "threshold_count_2.load()\n",
    "\n",
    "#for wetness_sensor3_nbar, filter, count, and load.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    water_plus_wetveg_3 = wetness_sensor3_nbar.where(wetness_sensor3_nbar>threshold)\n",
    "\n",
    "#count the number of wetness scenes for each pixel\n",
    "wet_count_3 = wetness_sensor3_nbar.count(dim='time')\n",
    "\n",
    "#count the amount of times that water plus wet veg is above the threshold\n",
    "threshold_count_3= water_plus_wetveg_3.count(dim='time')\n",
    "\n",
    "#bring both counts into memory\n",
    "wet_count_3.load()\n",
    "threshold_count_3.load()\n",
    "\n",
    "# #divide the number of times wetness is seen by the number of wetness scenes to get a proportion of time that the\n",
    "# #pixel is wet or wet veg'd:\n",
    "threshold_allsensors = threshold_count_1+threshold_count_2+threshold_count_3\n",
    "wet_count_allsensors = wet_count_1+ wet_count_2+ wet_count_3\n",
    "\n",
    "# #divide the number of times wetness is seen by the number of wetness scenes to get a proportion of time that the\n",
    "# #pixel is wet or wet veg'd:\n",
    "wet_proportion_allsensors= threshold_allsensors/wet_count_allsensors\n",
    "\n",
    "#turn array into dataset so we can write the netcdf\n",
    "dataset_tcw = wet_proportion_allsensors.to_dataset(name='tcw')\n",
    "\n",
    "#grab our crs attributes to write a spatially-referenced netcdf\n",
    "dataset_tcw.attrs['crs'] =  sensor1_nbar.crs\n",
    "dataset_tcw.tcw.attrs['crs'] =  sensor1_nbar.crs\n",
    "\n",
    "filename = netcdf_output_loc+shape_name+'.nc'\n",
    "try:\n",
    "    write_dataset_to_netcdf(dataset_tcw, filename)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError: {0}\".format(err))\n",
    "\n",
    "print('successfully ran TCI for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully ran TCI for '+shape_name+' polygon number '+str(i))\n",
    "\n",
    "#_____________this section added 04082017 to write the clear obs count ()\n",
    "\n",
    "#turn array into dataset so we can write the netcdf\n",
    "#overthresh is observations over our wetness threshold count per pixel\n",
    "dataset_overthresh = threshold_allsensors.to_dataset(name='overthresh')\n",
    "\n",
    "#grab our crs attributes to write a spatially-referenced netcdf\n",
    "dataset_overthresh.attrs['crs'] =  sensor1_nbar.crs\n",
    "dataset_overthresh.overthresh.attrs['crs'] =  sensor1_nbar.crs\n",
    "\n",
    "filename = netcdf_output_loc+shape_name+'_overthresh.nc'\n",
    "try:\n",
    "    write_dataset_to_netcdf(dataset_overthresh, filename)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError: {0}\".format(err))\n",
    "    \n",
    "print('successfully wrote overthresh netCDF for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote overthresh netCDFfor '+shape_name+' polygon number '+str(i))\n",
    "\n",
    "#turn array into dataset so we can write the netcdf\n",
    "#clear_observations is count of wetness scenes at pixel\n",
    "dataset_clearobs = wet_count_allsensors.to_dataset(name='clobs')\n",
    "\n",
    "#grab our crs attributes to write a spatially-referenced netcdf\n",
    "dataset_clearobs.attrs['crs'] =  sensor1_nbar.crs\n",
    "dataset_clearobs.clobs.attrs['crs'] =  sensor1_nbar.crs\n",
    "\n",
    "filename = netcdf_output_loc+shape_name+'_clearobs.nc'\n",
    "try:\n",
    "    write_dataset_to_netcdf(dataset_clearobs, filename)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError: {0}\".format(err))\n",
    "    \n",
    "print('successfully wrote clearobs netCDF for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote clearobs netCDFfor '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Storage Unit already exists: /g/data/r78/rjd547/groundwater_activities/Analysis/slice_0_overthresh.nc\n",
      "successfully wrote overthresh netCDF for slice_0 polygon number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "successfully wrote overthresh netCDFfor slice_0 polygon number 0\n"
     ]
    }
   ],
   "source": [
    "#turn array into dataset so we can write the netcdf\n",
    "#overthresh is observations over our wetness threshold count per pixel\n",
    "dataset_overthresh = threshold_allsensors.to_dataset(name='overthresh')\n",
    "\n",
    "#grab our crs attributes to write a spatially-referenced netcdf\n",
    "dataset_overthresh.attrs['crs'] =  sensor1_nbar.crs\n",
    "dataset_overthresh.overthresh.attrs['crs'] =  sensor1_nbar.crs\n",
    "\n",
    "filename = netcdf_output_loc+shape_name+'_overthresh.nc'\n",
    "try:\n",
    "    write_dataset_to_netcdf(dataset_overthresh, filename)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError: {0}\".format(err))\n",
    "    \n",
    "print('successfully wrote overthresh netCDF for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote overthresh netCDFfor '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Storage Unit already exists: /g/data/r78/rjd547/groundwater_activities/Analysis/slice_0_clearobs.nc\n",
      "successfully wrote clearobs netCDF for slice_0 polygon number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "successfully wrote clearobs netCDFfor slice_0 polygon number 0\n"
     ]
    }
   ],
   "source": [
    "#turn array into dataset so we can write the netcdf\n",
    "#clear_observations is count of wetness scenes at pixel\n",
    "dataset_clearobs = wet_count_allsensors.to_dataset(name='clobs')\n",
    "\n",
    "#grab our crs attributes to write a spatially-referenced netcdf\n",
    "dataset_clearobs.attrs['crs'] =  sensor1_nbar.crs\n",
    "dataset_clearobs.clobs.attrs['crs'] =  sensor1_nbar.crs\n",
    "\n",
    "filename = netcdf_output_loc+shape_name+'_clearobs.nc'\n",
    "try:\n",
    "    write_dataset_to_netcdf(dataset_clearobs, filename)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError: {0}\".format(err))\n",
    "    \n",
    "print('successfully wrote clearobs netCDF for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote clearobs netCDFfor '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dask_chunks': {'time': 5},\n",
       " 'geopolygon': Geometry(POLYGON ((131.554434234076 -13.8009653022795,131.547892015813 -13.8790036027955,131.496441846499 -13.8746177747694,131.503013176119 -13.7965809082819,131.554434234076 -13.8009653022795)), GEOGCS[\"GCS_WGS_1984\",DATUM[\"WGS_1984\",SPHEROID[\"WGS_84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]),\n",
       " 'time': ('1987-01-01', '2016-12-31')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
